## NumPy的應用-4

### 向量

**向量**（*vector*）也叫**矢量**，是一個同時具有大小和方向，且滿足平行四邊形法則的幾何對象。與向量相對的概念叫**標量**或**數量**，標量只有大小，絕大多數情況下沒有方向。我們通常用帶箭頭的線段來表示向量，在平面直角坐標系中的向量如下圖所示。需要注意的是，向量是表達大小和方向的量，並沒有規定起點和終點，所以相同的向量可以畫在任意位置，例如下圖中$\boldsymbol{w}$和$\boldsymbol{v}$兩個向量並沒有什麼區別。

<img src="res/vector_1.png" style="zoom:40%;">

向量有很多種代數表示法，對於二維空間的向量，下面幾種寫法都是可以的。
$$
\boldsymbol{a} = \langle a_1, a_2 \rangle = (a_1, a_2) = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix} = \begin{bmatrix} a_1 \\ a_2 \end{bmatrix}
$$
向量的大小稱為向量的模，它是一個標量，對於二維空間的向量，模可以通過下面的公式計算。
$$
\lvert \boldsymbol{a} \rvert = \sqrt{a_{1}^{2} + a_{2}^{2}}
$$
注意，這里的$\lvert \boldsymbol{a} \rvert$並不是絕對值，你可以將其稱為向量$\boldsymbol{a}$的二範數，這是數學中的符號重用現象。上面的寫法和概念也可以推廣到$n$維空間，我們通常用$\boldsymbol{R^n}$表示$n$維空間，我們剛才說的二維空間可以記為$\boldsymbol{R^2}$，三維空間可以記為$\boldsymbol{R^3}$。雖然生活在三維空間的我們很難想象四維空間、五維空間是什麼樣子，但是這並不影響我們探討高維空間，機器學習中，我們經常把有$n$個特徵的訓練樣本稱為一個$n$維向量。

#### 向量的加法

相同維度的向量可以相加得到一個新的向量，運算的方法是將向量的每個分量相加，如下所示。
$$
\boldsymbol{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}, \quad
\boldsymbol{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}, \quad
\boldsymbol{u} + \boldsymbol{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}
$$
向量的加法滿足“平行四邊形法則”，即兩個向量$\boldsymbol{u}$和$\boldsymbol{v}$構成了平行四邊形的兩條鄰邊，相加的結果是平行四邊形的對角線，如下圖所示。

<img src="res/vector_2.png" style="zoom:58%;">

#### 向量的數乘

一個向量$\boldsymbol{v}$可以和一個標量$k$相乘，運算的方法是將向量中的每個分量與該標量相乘即可，如下所示。
$$
\boldsymbol{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}, \quad
k \cdot \boldsymbol{v} = \begin{bmatrix} k \cdot v_1 \\ k \cdot v_2 \\ \vdots \\ k \cdot v_n \end{bmatrix}
$$
我們可以用 NumPy 的數組來表示向量，向量的加法可以通過兩個數組的加法來實現，向量的數乘可以通過數組和標量的乘法來實現，此處不再進行贅述。

#### 向量的點積

點積（*dot product*）是兩個向量之間最為重要的運算之一，運算的方法是將兩個向量對應分量的乘積求和，所以點積的結果是一個標量，其幾何意義是兩個向量的模乘以二者夾角的餘弦如下所示。
$$
\boldsymbol{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}, \quad
\boldsymbol{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} \quad \\
\boldsymbol{u} \cdot \boldsymbol{v} = \sum_{i=1}^{n}{u_iv_i} = \lvert \boldsymbol{u} \rvert \lvert \boldsymbol{v} \rvert cos\theta
$$
假如我們用3維向量來表示用戶對喜劇片、言情片和動作片這三類電影的偏好，我們用1到5的數字來表示喜歡的程度，其中5表示非常喜歡，4表示比較喜歡，3表示無感，2表示比較反感，1表示特別反感。那麼，下面的向量表示用戶非常喜歡喜劇片，特別反感言情片，對動作片不喜歡也不反感。
$$
\boldsymbol{u} = \begin{pmatrix} 5 \\ 1 \\ 3 \end{pmatrix}
$$
現在有兩部電影上映了，一部屬於言情喜劇片，一部屬於喜劇動作片，我們把兩部電影也通過3維向量的方式進行表示，如下所示。
$$
\boldsymbol{m_1} = \begin{pmatrix} 4 \\ 5 \\ 1 \end{pmatrix}, \quad \boldsymbol{m_2} = \begin{pmatrix} 5 \\ 1 \\ 5 \end{pmatrix}
$$
如果現在我們需要向剛才的用戶推薦一部電影，我們應該給他推薦哪一部呢？我們可以將代表用戶的向量$\boldsymbol{u}$和代表電影的向量$\boldsymbol{m_1}$和$\boldsymbol{m_2}$分別進行點積運算，再除以向量的模長，得到向量夾角的餘弦值，餘弦值越接近1，說明向量的夾角越接近0度，也就是兩個向量的相似度越高。很顯然，我們應該向用戶推薦跟他觀影喜好相似度更高的電影。
$$
cos\theta_1 = \frac{\boldsymbol{u} \cdot \boldsymbol{m1}}{|\boldsymbol{u}||\boldsymbol{m1}|} \approx \frac{4 \times 5 + 5 \times 1 + 3 \times 1}{5.92 \times 6.48} \approx 0.73 \\
cos\theta_2 = \frac{\boldsymbol{u} \cdot \boldsymbol{m2}}{|\boldsymbol{u}||\boldsymbol{m2}|} \approx \frac{5 \times 5 + 1 \times 1 + 3 \times 5}{5.92 \times 7.14} \approx 0.97
$$
大家可能會說，向量$\boldsymbol{m_2}$代表的電影肉眼可見跟用戶是更加匹配的。的確，對於一個三維向量我們憑借直覺也能夠給出正確的答案，但是對於一個$n$維向量，當$n$的值非常大時，你還有信心憑借肉眼的觀察和本能的直覺給出準確的答案嗎？向量的點積可以通過`dot`函數來計算，而向量的模長可以通過 NumPy 的`linalg`模塊中的`norm`函數來計算，代碼如下所示。

```python
u = np.array([5, 1, 3])
m1 = np.array([4, 5, 1])
m2 = np.array([5, 1, 5])
print(np.dot(u, m1) / (np.linalg.norm(u) * np.linalg.norm(m1)))  # 0.7302967433402214
print(np.dot(u, m2) / (np.linalg.norm(u) * np.linalg.norm(m2)))  # 0.9704311900788593
```

#### 向量的叉積

在二維空間，兩個向量的叉積是這樣定義的：
$$
\boldsymbol{A} = \begin{pmatrix} a_{1} \\ a_{2} \end{pmatrix}, \quad \boldsymbol{B} = \begin{pmatrix} b_{1} \\ b_{2} \end{pmatrix} \\
\boldsymbol{A} \times \boldsymbol{B} = \begin{vmatrix} a_{1} \quad a_{2} \\ b_{1} \quad b_{2} \end{vmatrix} = a_{1}b_{2} - a_{2}b_{1}
$$
對於三維空間，兩個向量的叉積結果是一個向量，如下所示：
$$
\boldsymbol{A} = \begin{pmatrix} a_{1} \\ a_{2} \\ a_{3} \end{pmatrix}, \quad \boldsymbol{B} = \begin{pmatrix} b_{1} \\ b_{2} \\ b_{3} \end{pmatrix} \\
\boldsymbol{A} \times \boldsymbol{B} = \begin{vmatrix} \boldsymbol{\hat{i}} \quad \boldsymbol{\hat{j}} \quad \boldsymbol{\hat{k}} \\ a_{1} \quad a_{2} \quad a_{3} \\ b_{1} \quad b_{2} \quad b_{3} \end{vmatrix} = \langle \boldsymbol{\hat{i}}\begin{vmatrix} a_{2} \quad a_{3} \\ b_{2} \quad b_{3} \end{vmatrix}, -\boldsymbol{\hat{j}}\begin{vmatrix} a_{1} \quad a_{3} \\ b_{1} \quad b_{3} \end{vmatrix}, \boldsymbol{\hat{k}}\begin{vmatrix} a_{1} \quad a_{2} \\ b_{1} \quad b_{2} \end{vmatrix} \rangle
$$
因為叉積的結果是向量，所以$\boldsymbol{A} \times \boldsymbol{B}$和$\boldsymbol{B} \times \boldsymbol{A}$的結果並不相同，事實上：
$$
\boldsymbol{A} \times \boldsymbol{B} = -(\boldsymbol{B} \times \boldsymbol{A})
$$
NumPy 中可以通過`cross`函數來計算向量的叉積，代碼如下所示。

```python
print(np.cross(u, m1))  # [-14   7  21]
print(np.cross(m1, u))  # [ 14  -7 -21]
```

### 行列式

**行列式**（*determinant*）通常記作$det(\boldsymbol{A})$或$|\boldsymbol{A}|$，其中$\boldsymbol{A}$是一個$n$階方陣。行列式可以看做是有向面積或體積的概念在一般歐幾里得空間的推廣，或者說行列式描述的是一個線性變換對“體積”所造成的影響。行列式的概念最早出現在解線性方程組的過程中，十七世紀晚期，關孝和（日本江戶時代的數學家）與萊佈尼茨的著作中已經使用行列式來確定線性方程組解的個數以及形式；十八世紀開始，行列式開始作為獨立的數學概念被研究；十九世紀以後，行列式理論進一步得到發展和完善。

<img src="res/Parallelogramme.jpeg" style="zoom:125%;">

#### 行列式的性質

行列式是由向量引出的，所以行列式解釋的其實是向量的性質。

**性質1**：如果$det(\boldsymbol{A})$中某行（或某列）的元素全部為0，那麼$det(\boldsymbol{A}) = 0$。

**性質2**：如果$det(\boldsymbol{A})$中某行（或某列）有公共因子$k$，則可以提出$k$，得到行列式$det(\boldsymbol{A^{'}})$，且$det(\boldsymbol{A}) = k \cdot det(\boldsymbol{A^{'}})$。
$$
det(\boldsymbol{A})={\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {blue}k}a_{i1}&{\color {blue}k}a_{i2}&\dots &{\color {blue}k}a_{in}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\color {blue}k}{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\a_{i1}&a_{i2}&\dots &a_{in}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\color {blue}k} \cdot det(\boldsymbol{A^{'}})
$$

**性質3**：如果$det(\boldsymbol{A})$中某行（或某列）的每個元素是兩數之和，則此行列式可拆分為兩個行列式相加，如下所示。
$$
{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {blue}a_{i1}}+{\color {OliveGreen}b_{i1}}&{\color {blue}a_{i2}}+{\color {OliveGreen}b_{i2}}&\dots &{\color {blue}a_{in}}+{\color {OliveGreen}b_{in}}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {blue}a_{i1}}&{\color {blue}a_{i2}}&\dots &{\color {blue}a_{in}}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}+{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {OliveGreen}b_{i1}}&{\color {OliveGreen}b_{i2}}&\dots &{\color {OliveGreen}b_{in}}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}
$$
**性質4**：如果$det(\boldsymbol{A})$中兩行（或兩列）元素對應成比例，那麼$det(\boldsymbol{A}) = 0$。

**性質5**：如果$det(\boldsymbol{A})$中兩行（或兩列）互換得到$det(\boldsymbol{A^{'}})$，那麼$det(\boldsymbol{A}) = -det(\boldsymbol{A^{'}})$。

**性質6**：將$det(\boldsymbol{A})$中某行（或某列）的$k$倍加進另一行（或另一列）里，行列式的值不變，如下所示。
$$
{\begin{vmatrix}\vdots &\vdots &\vdots &\vdots \\a_{i1}&a_{i2}&\dots &a_{in}\\a_{j1}&a_{j2}&\dots &a_{jn}\\\vdots &\vdots &\vdots &\vdots \\\end{vmatrix}}={\begin{vmatrix}\vdots &\vdots &\vdots &\vdots \\a_{i1}&a_{i2}&\dots &a_{in}\\a_{j1}{\color {blue}+ka_{i1}}&a_{j2}{\color {blue}+ka_{i2}}&\dots &a_{jn}{\color {blue}+ka_{in}}\\\vdots &\vdots &\vdots &\vdots \\\end{vmatrix}}
$$
**性質7**：將行列式的行列互換，行列式的值不變，如下所示。
$$
{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\a_{21}&a_{22}&\dots &a_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\begin{vmatrix}a_{11}&a_{21}&\dots &a_{n1}\\a_{12}&a_{22}&\dots &a_{n2}\\\vdots &\vdots &\ddots &\vdots \\a_{1n}&a_{2n}&\dots &a_{nn}\end{vmatrix}}
$$
**性質8**：方塊矩陣$\boldsymbol{A}$和$\boldsymbol{B}$的乘積的行列式等於其行列式的乘積，即$det(\boldsymbol{A}\boldsymbol{B}) = det(\boldsymbol{A})det(\boldsymbol{B})$。特別的，若將矩陣中的每一行都乘以常數$r$，那麼行列式的值將是原來的$r^{n}$倍，即$det(r\boldsymbol{A}) = det(r\boldsymbol{I_{n}} \cdot \boldsymbol{A}) = r^{n}det(\boldsymbol{A})$，其中$\boldsymbol{I_{n}}$是$n$階單位矩陣。

**性質9**：若$\boldsymbol{A}$是可逆矩陣，那麼$det(\boldsymbol{A}^{-1}) = (det(\boldsymbol{A}))^{-1}$。

#### 行列式的計算

$n$階行列式的計算公式如下所示：
$$
det(\boldsymbol{A})=\sum_{n!} \pm {a_{1\alpha}a_{2\beta} \cdots a_{n\omega}}
$$

對於二階行列式，上面的公式相當於：
$$
\begin{vmatrix} a_{11} \quad a_{12} \\ a_{21} \quad a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a_{21}
$$
對於三階行列式，上面的計算公式相當於：
$$
\begin{vmatrix} a_{11} \quad a_{12} \quad a_{13} \\ a_{21} \quad a_{22} \quad a_{23} \\ a_{31} \quad a_{32} \quad a_{33} \end{vmatrix} = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}
$$
高階行列式可以用**代數餘子式**（*cofactor*）展開成多個低階行列式，如下所示：
$$
det(\boldsymbol{A})=a_{11}C_{11}+a_{12}C_{12}+ \cdots +a_{1n}C_{1n} = \sum_{i=1}^{n}{a_{1i}C_{1i}}
$$
其中，$C_{11}$是原行列式去掉$a_{11}$所在行和列之後剩餘的部分構成的行列式，以此類推。

### 矩陣

**矩陣**（*matrix*）是由一系列元素排成的矩形陣列，矩陣里的元素可以是數字、符號或數學公式。矩陣可以進行**加法**、**減法**、**數乘**、**轉置**、**矩陣乘法**等運算，如下圖所示。

<img src="res/matrix_operation.png" style="zoom:62%;">

值得一提的是矩陣乘法運算，該運算僅當第一個矩陣$\boldsymbol{A}$的列數和另一個矩陣$\boldsymbol{B}$的行數相等時才能定義。如果$\boldsymbol{A}$是一個$m \times n$的矩陣，$\boldsymbol{B}$是一個$n \times k$矩陣，它們的乘積是一個$m \times k$的矩陣，其中元素的計算公式如下所示：
$$
 [\mathbf{AB}]_{i,j} = A_{i,1}B_{1,j} + A_{i,2}B_{2,j} + \cdots + A_{i,n}B_{n,j} = \sum_{r=1}^n A_{i,r}B_{r,j}
$$
<img src="res/matrix_multiply.png" style="zoom:35%;">

例如：
$$
\begin{bmatrix}
    1 & 0 & 2 \\
    -1 & 3 & 1 \\
  \end{bmatrix}
\times
  \begin{bmatrix}
    3 & 1 \\
    2 & 1 \\
    1 & 0
  \end{bmatrix}
=
  \begin{bmatrix}
     (1 \times 3  +  0 \times 2  +  2 \times 1) & (1 \times 1   +   0 \times 1   +   2 \times 0) \\
    (-1 \times 3  +  3 \times 2  +  1 \times 1) & (-1 \times 1   +   3 \times 1   +   1 \times 0) \\
  \end{bmatrix}
=
  \begin{bmatrix}
    5 & 1 \\
    4 & 2 \\
  \end{bmatrix}
$$
矩陣的乘法滿足結合律和對矩陣加法的分配律：

結合律： $(\boldsymbol{AB})\boldsymbol{C} = \boldsymbol{A}(\boldsymbol{BC})$。

左分配律：$(\boldsymbol{A} + \boldsymbol{B})\boldsymbol{C} = \boldsymbol{AC} + \boldsymbol{BC}$。

右分配律：$\boldsymbol{C}(\boldsymbol{A} + \boldsymbol{B}) = \boldsymbol{CA} + \boldsymbol{CB}$。

**矩陣乘法不滿足交換律**。一般情況下，矩陣$\boldsymbol{A}$和$\boldsymbol{B}$的乘積$\boldsymbol{AB}$存在，但$\boldsymbol{BA}$不一定存在，即便$\boldsymbol{BA}$存在，大多數時候$\boldsymbol{AB} \neq \boldsymbol{BA}$。

矩陣乘法的一個基本應用是在線性方程組上。線性方程組是方程組的一種，它符合以下的形式：
$$
\begin{cases}
     a_{1,1}x_{1} + a_{1,2}x_{2} + \cdots + a_{1,n}x_{n}=  b_{1} \\
     a_{2,1}x_{1} + a_{2,2}x_{2} + \cdots + a_{2,n}x_{n}=  b_{2} \\
     \vdots \quad \quad \quad \vdots \\
     a_{m,1}x_{1} + a_{m,2}x_{2} + \cdots + a_{m,n}x_{n}=  b_{m}
 \end{cases}
$$
運用矩陣的方式，可以將線性方程組寫成一個向量方程：
$$
\boldsymbol{Ax} = \boldsymbol{b}
$$
其中，$\boldsymbol{A}$是由方程組里未知數的系數排成的$m \times n$矩陣，$\boldsymbol{x}$是含有$n$個元素的行向量，$\boldsymbol{b}$是含有$m$個元素的行向量。

矩陣是線性變換（保持向量加法和標量乘法的函數）的便利表達法。矩陣乘法的本質在聯系到線性變換的時候最能體現，因為矩陣乘法和線性變換的合成有以下的聯系，即每個$m \times n$的矩陣$\boldsymbol{A}$都代表了一個從$\boldsymbol{R}^{n}$射到$\boldsymbol{R}^{m}$的線性變換。如果無法理解上面這些內容，推薦大家看看B站上名為[《線性代數的本質》](https://www.bilibili.com/video/BV1ib411t7YR/)的視頻，相信這套視頻會讓你對線性代數有一個更好的認知。

下圖是一個來自於維基百科的例子，圖中展示了一些典型的二維實平面上的線性變換對平面向量（圖形）造成的效果以及它們對應的二維矩陣，其中每個線性變換將藍色圖形映射成綠色圖形；平面的原點$(0, 0)$用黑點表示。

<img src="res/linear_transformation.png" style="zoom:45%;">

#### 矩陣對象

NumPy 中提供了專門用於線性代數（*linear algebra*）的模塊和表示矩陣的類型`matrix`，當然我們通過二維數組也可以表示一個矩陣，官方並不推薦使用`matrix`類而是建議使用二維數組，而且有可能在將來的版本中會移除`matrix`類。無論如何，利用這些已經封裝好的類和函數，我們可以輕松愉快的實現很多對矩陣的操作。

我們可以通過下面的代碼來創建矩陣（`matrix`）對象。

代碼：

```Python
m1 = np.matrix('1 2 3; 4 5 6')
m1
```

> **說明**：`matrix`構造器可以傳入類數組對象也可以傳入字符串來構造矩陣對象。

輸出：

```
matrix([[1, 2, 3],
        [4, 5, 6]])
```

代碼：

```Python
m2 = np.asmatrix(np.array([[1, 1], [2, 2], [3, 3]]))
m2
```

> **說明**：`asmatrix`函數也可以用`mat`函數代替，這兩個函數其實是同一個函數。

輸出：

```
matrix([[1, 1],
        [2, 2],
        [3, 3]])
```

代碼：

```Python
m1 * m2
```

輸出：

```
matrix([[14, 14],
        [32, 32]])
```

> **說明**：注意`matrix`對象和`ndarray`對象乘法運算的差別，`matrix`對象的`*`運算是矩陣乘法運算。如果兩個二維數組要做矩陣乘法運算，應該使用`@`運算符或`matmul`函數，而不是`*`運算符。

矩陣對象的屬性如下表所示。

| 屬性    | 說明                                      |
| ------- | ----------------------------------------- |
| `A`     | 獲取矩陣對象對應的`ndarray`對象           |
| `A1`    | 獲取矩陣對象對應的扁平化後的`ndarray`對象 |
| `I`     | 可逆矩陣的逆矩陣                          |
| `T`     | 矩陣的轉置                                |
| `H`     | 矩陣的共軛轉置                            |
| `shape` | 矩陣的形狀                                |
| `size`  | 矩陣元素的個數                            |

矩陣對象的方法跟之前講過的`ndarray`數組對象的方法基本差不多，此處不再進行贅述。

#### 線性代數模塊

NumPy 的`linalg`模塊中有一組標準的矩陣分解運算以及諸如求逆和行列式之類的函數，它們跟 MATLAB 和 R 等語言所使用的是相同的行業標準線性代數庫，下面的表格列出了`numpy`以及`linalg`模塊中一些常用的線性代數相關函數。

| 函數          | 說明                                                         |
| ------------- | ------------------------------------------------------------ |
| `diag`        | 以一維數組的形式返回方陣的對角線元素或將一維數組轉換為方陣（非對角元素元素為0） |
| `matmul`      | 矩陣乘法運算                                                 |
| `trace`       | 計算對角線元素的和                                           |
| `norm`        | 求矩陣或向量的範數                                           |
| `det`         | 計算行列式的值                                               |
| `matrix_rank` | 計算矩陣的秩                                                 |
| `eig`         | 計算矩陣的特徵值（*eigenvalue*）和特徵向量（*eigenvector*）  |
| `inv`         | 計算非奇異矩陣（$n$階方陣）的逆矩陣                          |
| `pinv`        | 計算矩陣的摩爾-彭若斯（*Moore-Penrose*）廣義逆               |
| `qr`          | QR分解（把矩陣分解成一個正交矩陣與一個上三角矩陣的積）       |
| `svd`         | 計算奇異值分解（*singular value decomposition*）             |
| `solve`       | 解線性方程組$\boldsymbol{Ax}=\boldsymbol{b}$，其中$\boldsymbol{A}$是一個方陣 |
| `lstsq`       | 計算$\boldsymbol{Ax}=\boldsymbol{b}$的最小二乘解             |

下面我們簡單嘗試一下上面的函數，先試一試求逆矩陣。

代碼：

```Python
m3 = np.array([[1., 2.], [3., 4.]])
m4 = np.linalg.inv(m3)
m4
```

輸出：

```
array([[-2. ,  1. ],
       [ 1.5, -0.5]])
```

代碼：

```Python
np.around(m3 @ m4)
```

> **說明**：`around`函數對數組元素進行四舍五入操作，默認小數點後面的位數為0。

輸出：

```
array([[1., 0.],
       [0., 1.]])
```

> **說明**：矩陣和它的逆矩陣做矩陣乘法會得到單位矩陣。

計算行列式的值。

代碼：

```Python
m5 = np.array([[1, 3, 5], [2, 4, 6], [4, 7, 9]])
np.linalg.det(m5)
```

輸出：

```
2
```

計算矩陣的秩。

代碼：

```Python
np.linalg.matrix_rank(m5)
```

輸出：

```
3
```

求解線性方程組。
$$
\begin{cases}
x_1 + 2x_2 + x_3 = 8 \\
3x_1 + 7x_2 + 2x_3 = 23 \\
2x_1 + 2x_2 + x_3 = 9
\end{cases}
$$

對於上面的線性方程組，我們可以用矩陣的形式來表示它，如下所示。
$$
\boldsymbol{A} = \begin{bmatrix}
1 & 2 & 1\\
3 & 7 & 2\\
2 & 2 & 1
\end{bmatrix}, \quad
\boldsymbol{x} = \begin{bmatrix}
x_1 \\
x_2\\
x_3
\end{bmatrix}, \quad
\boldsymbol{b} = \begin{bmatrix}
8 \\
23\\
9
\end{bmatrix}
$$

$$
\boldsymbol{Ax} = \boldsymbol{b}
$$

線性方程組有唯一解的條件：系數矩陣$\boldsymbol{A}$的秩等於增廣矩陣$\boldsymbol{Ab}$的秩，而且跟未知數的個數相同。

代碼：

```Python
A = np.array([[1, 2, 1], [3, 7, 2], [2, 2, 1]])
b = np.array([8, 23, 9]).reshape(-1, 1)
print(np.linalg.matrix_rank(A))
print(np.linalg.matrix_rank(np.hstack((A, b))))
```

> **說明**：使用數組對象的`reshape`方法調形時，如果其中一個參數為-1，那麼該維度有多少個元素是通過數組元素個數（`size`屬性）和其他維度的元素個數自動計算出來的。

輸出：

```
3
3
```

代碼：

```Python
np.linalg.solve(A, b)
```

輸出：

```
array([[1.],
       [2.],
       [3.]])
```

> **說明**：上面的結果表示，線性方程組的解為：$x_1 = 1, x_2 = 2, x_3 = 3$。

下面是另一種求解線性方程組的方法，大家可以停下來思考下為什麼。
$$
\boldsymbol{x} = \boldsymbol{A}^{-1} \cdot \boldsymbol{b}
$$
代碼：

```Python
np.linalg.inv(A) @ b
```

輸出：

```
array([[1.],
       [2.],
       [3.]])
```

### 多項式

除了數組，NumPy 中還封裝了用於**多項式**（*polynomial*）運算的數據類型。多項式是變量的整數次冪與系數的乘積之和，形如：
$$
f(x)=a_nx^n + a_{n-1}x^{n-1} + \cdots + a_1x^{1} + a_0x^{0}
$$
在 NumPy 1.4版本之前，我們可以用`poly1d`類型來表示多項式，目前它仍然可用，但是官方提供了新的模塊`numpy.polynomial`，它除了支持基本的冪級數多項式外，還可以支持切比雪夫多項式、拉蓋爾多項式等。

#### 創建多項式對象

創建`poly1d`對象，例如：$\small{f(x)=3x^{2}+2x+1}$。

代碼：

```python
p1 = np.poly1d([3, 2, 1])
p2 = np.poly1d([1, 2, 3])
print(p1)
print(p2)
```

輸出：

```
   2
3 x + 2 x + 1
   2
1 x + 2 x + 3
```

#### 多項式的操作

**獲取多項式的系數**

代碼：

```python
print(p1.coefficients)
print(p2.coeffs)
```

輸出：

```
[3 2 1]
[1 2 3]
```

**兩個多項式的四則運算**

代碼：

```python
print(p1 + p2)
print(p1 * p2)
```

輸出：

```
   2
4 x + 4 x + 4
   4     3      2
3 x + 8 x + 14 x + 8 x + 3
```

**帶入$\small{x}$求多項式的值**

代碼：

```python
print(p1(3))
print(p2(3))
```

輸出：

```
34
18
```

**多項式求導和不定積分**

代碼：

```python
print(p1.deriv())
print(p1.integ())
```

輸出：

```

6 x + 2
   3     2
1 x + 1 x + 1 x
```

**求多項式的根**

例如有多項式$\small{f(x)=x^2+3x+2}$，多項式的根即一元二次方程$\small{x^2+3x+2=0}$的解。

代碼：

```python
p3 = np.poly1d([1, 3, 2])
print(p3.roots)
```

輸出：

```
[-2. -1.]
```

如果使用`numpy.polynomial`模塊的`Polynomial`類來表示多項式對象，那麼對應的操作如下所示。

代碼：

```python
from numpy.polynomial import Polynomial

p3 = Polynomial((2, 3, 1))
print(p3)           # 輸出多項式
print(p3(3))        # 令x=3，計算多項式的值
print(p3.roots())   # 計算多項式的根
print(p3.degree())  # 獲得多項式的次數
print(p3.deriv())   # 求導
print(p3.integ())   # 求不定積分
```

輸出：

```
2.0 + 3.0·x + 1.0·x²
20.0
[-2. -1.]
2
3.0 + 2.0·x
0.0 + 2.0·x + 1.5·x² + 0.33333333·x³
```

#### 最小二乘解

`Polynomial`類還有一個名為`fit`的類方法，它可以給多項式求最小二乘解。所謂最小二乘解（least-squares solution），是用最小二乘法通過最小化誤差的平方和來尋找數據的最佳匹配函數的系數。假設多項式為$\small{f(x)=ax+b}$，最小二乘解就是讓下面的殘差平方和$\small{RSS}$達到最小的$\small{a}$和$\small{b}$。
$$
RSS = \sum_{i=0}^{k}(f(x_i) - y_i)^{2}
$$
例如，我們想利用收集到的月收入和網購支出的歷史數據來建立一個預測模型，以達到通過某人的月收入預測他網購支出金額的目標，下面是我們收集到的收入和網購支出的數據，保存在兩個數組中。

```python
x = np.array([
    25000, 15850, 15500, 20500, 22000, 20010, 26050, 12500, 18500, 27300,
    15000,  8300, 23320,  5250,  5800,  9100,  4800, 16000, 28500, 32000,
    31300, 10800,  6750,  6020, 13300, 30020,  3200, 17300,  8835,  3500
])
y = np.array([
    2599, 1400, 1120, 2560, 1900, 1200, 2320,  800, 1650, 2200,
     980,  580, 1885,  600,  400,  800,  420, 1380, 1980, 3999,
    3800,  725,  520,  420, 1200, 4020,  350, 1500,  560,  500
])
```

我們可以先繪制散點圖來了解兩組數據是否具有正相關或負相關關系。正相關意味着數組`x`中較大的值對應到數組`y`中也是較大的值，而負相關則意味着數組`x`中較大的值對應到數組`y`中較小的值。

```python
import matplotlib.pyplot as plt

plt.figure(dpi=120)
plt.scatter(x, y, color='blue')
plt.show()
```

輸出：

<img src="res/in_out_scatter_plot.png" style="zoom:50%;">

如果需要定量的研究兩組數據的相關性，我們可以計算協方差或相關系數，對應的 NumPy 函數分別是`cov`和`corrcoef`。

代碼：

```python
np.corrcoef(x, y)
```

輸出：

```
array([[1.        , 0.92275889],
       [0.92275889, 1.        ]])
```

> **說明**：相關系數是一個`-1`到`1`之間的值，越靠近`1` 說明正相關性越強，越靠近`-1`說明負相關性越強，靠近`0`則說明兩組數據沒有明顯的相關性。上面月收入和網購支出之間的相關系數是`0.92275889`，說明二者是強正相關關系。

通過上面的操作，我們確定了收入和網購支出之前存在強正相關關系，於是我們用這些數據來創建一個回歸模型，找出一條能夠很好的擬合這些數據點的直線。這里，我們就可以用到上面提到的`fit`方法，具體的代碼如下所示。

代碼：

```python
from numpy.polynomial import Polynomial

Polynomial.fit(x, y, deg=1).convert().coef
```

> **說明**：`deg=1`說明回歸模型最高次項就是1次項，回歸模型形如$\small{y=ax+b}$；如果要生一個類似於$\small{y=ax^2+bx+c}$的模型，就需要設置`deg=2`，以此類推。

輸出：

```
array([-2.94883437e+02,  1.10333716e-01])
```

根據上面輸出的結果，我們的回歸方程應該是$\small{y=0.110333716x-294.883437}$。我們將這個回歸方程繪制到剛才的散點圖上，紅色的點是我們的預測值，藍色的點是歷史數據，也就是真實值。

代碼：

```python
import matplotlib.pyplot as plt

plt.scatter(x, y, color='blue')
plt.scatter(x, 0.110333716 * x - 294.883437, color='red')
plt.plot(x, 0.110333716 * x - 294.883437, color='darkcyan')
plt.show()
```

輸出：

<img src="res/in_out_regression_result.png" style="zoom:50%;">

如果不使用`Polynomial`類型的`fit`方法，我們也可以通過 NumPy 提供的`polyfit`函數來完成同樣的操作，有興趣的讀者可以自行研究。