## 深入淺出pandas-3

在完成數據加載之後，我們可能需要對事實表和維度表進行連接，這是對數據進行多維度拆解的基礎；我們可能從不同的數據源加載了結構相同的數據，我們需要將這些數據拼接起來；我們把這些操作統稱為數據重塑。當然，由於企業的信息化水平以及數據中台建設水平的差異，我們拿到的數據未必是質量很好的，可能還需要對數據中的缺失值、重複值、異常值進行適當的處理。即便我們獲取的數據在質量上是沒有問題的，但也可能需要對數據進行一系列的預處理，才能滿足我們做數據分析的需求。接下來，我們就為大家講解和梳理這方面的知識。

### 數據重塑

有的時候，我們做數據分析需要的原始數據可能並不是來自一個地方，就像上一章的例子中，我們從關系型數據庫中讀取了三張表，得到了三個`DataFrame`對象，但實際工作可能需要我們把他們的數據整合到一起。例如：`emp_df`和`emp2_df`其實都是員工的數據，而且數據結構完全一致，我們可以使用`pandas`提供的`concat`函數實現兩個或多個`DataFrame`的數據拼接，代碼如下所示。

```Python
all_emp_df = pd.concat([emp_df, emp2_df])
```

輸出：

```
        ename    job        mgr      sal     comm    dno
eno
1359    胡一刀    銷售員	   3344.0	1800	200.0	30
2056    喬峰	    分析師	    7800.0	 5000	 1500.0	 20
3088    李莫愁	   設計師	   2056.0	3500	800.0	20
3211    張無忌	   程序員	   2056.0	3200	NaN     20
3233    丘處機	   程序員	   2056.0	3400	NaN	    20
3244    歐陽鋒	   程序員	   3088.0	3200	NaN     20
3251    張翠山	   程序員	   2056.0	4000	NaN	    20
3344    黃蓉	    銷售主管   7800.0	3000	800.0	30
3577    楊過	    會計	     5566.0	  2200	  NaN	  10
3588    朱九真	   會計	    5566.0	 2500	 NaN	 10
4466    苗人鳳	   銷售員	   3344.0	2500	NaN	    30
5234    郭靖	    出納	     5566.0	  2000	  NaN	  10
5566    宋遠橋	   會計師	   7800.0	4000	1000.0	10
7800    張三豐	   總裁	    NaN      9000	 1200.0	 20
9500	張三豐	   總裁	    NaN	     50000	 8000.0	 20
9600	王大錘    程序員	   9800.0	8000	600.0	20
9700	張三豐	   總裁	    NaN	     60000	 6000.0	 20
9800	駱昊	    架構師	    7800.0	 30000	 5000.0	 20
9900	陳小刀	   分析師	   9800.0	10000	1200.0	20
```

上面的代碼將兩個代表員工數據的`DataFrame`拼接到了一起，接下來我們使用`merge`函數將員工表和部門表的數據合並到一張表中，代碼如下所示。

先使用`reset_index`方法重新設置`all_emp_df`的索引，這樣`eno` 不再是索引而是一個普通列，`reset_index`方法的`inplace`參數設置為`True`表示，重置索引的操作直接在`all_emp_df`上執行，而不是返回修改後的新對象。

```Python
all_emp_df.reset_index(inplace=True)
```

通過`merge`函數合並數據，當然，也可以調用`DataFrame`對象的`merge`方法來達到同樣的效果。

```Python
pd.merge(all_emp_df, dept_df, how='inner', on='dno')
```

輸出：

```
    eno	    ename	job	     mgr	 sal	 comm	 dno	dname	 dloc
0	1359	胡一刀	 銷售員	3344.0	1800	200.0	30	   銷售部	 重慶
1	3344	黃蓉	  銷售主管	7800.0	3000	800.0	30	   銷售部	 重慶
2	4466	苗人鳳	 銷售員	3344.0	2500	NaN	    30	   銷售部	 重慶
3	2056	喬峰	  分析師	 7800.0	 5000	 1500.0	 20	    研發部	  成都
4	3088	李莫愁	 設計師	2056.0	3500	800.0	20	   研發部	 成都
5	3211	張無忌  程序員	2056.0	3200	NaN	    20	   研發部	 成都
6	3233	丘處機	 程序員	2056.0	3400	NaN	    20	   研發部	 成都
7	3244	歐陽鋒	 程序員	3088.0	3200	NaN	    20	   研發部	 成都
8	3251	張翠山	 程序員	2056.0	4000	NaN	    20	   研發部	 成都
9	7800	張三豐	 總裁	     NaN	 9000	 1200.0	 20	    研發部	  成都
10	9500	張三豐	 總裁	     NaN	 50000	 8000.0	 20	    研發部	  成都
11	9600	王大錘	 程序員	9800.0	8000	600.0	20	   研發部	 成都
12	9700	張三豐	 總裁	     NaN	 60000	 6000.0	 20	    研發部	  成都
13	9800	駱昊	  架構師	 7800.0	 30000	 5000.0	 20	    研發部	  成都
14	9900	陳小刀	 分析師	9800.0	10000	1200.0	20	   研發部	 成都
15	3577	楊過	  會計	  5566.0  2200	  NaN	  10	會計部	  北京
16	3588	朱九真	 會計	     5566.0	 2500	 NaN	 10	   會計部	 北京
17	5234	郭靖	  出納	  5566.0  2000	  NaN	  10	會計部	  北京
18	5566	宋遠橋	 會計師	7800.0	4000	1000.0	10	  會計部	北京
```

`merge`函數的一個參數代表合並的左表、第二個參數代表合並的右表，有SQL編程經驗的同學對這兩個詞是不是感覺到非常親切。正如大家猜想的那樣，`DataFrame`對象的合並跟數據庫中的表連接非常類似，所以上面代碼中的`how`代表了合並兩張表的方式，有`left`、`right`、`inner`、`outer`四個選項；而`on`則代表了基於哪個列實現表的合並，相當於 SQL 表連接中的連表條件，如果左右兩表對應的列列名不同，可以用`left_on`和`right_on`參數取代`on`參數分別進行指定。

如果對上面的代碼稍作修改，將`how`參數修改為`'right'`，大家可以思考一下代碼執行的結果。

```Python
pd.merge(all_emp_df, dept_df, how='right', on='dno')
```

運行結果比之前的輸出多出了如下所示的一行，這是因為`how='right'`代表右外連接，也就意味着右表`dept_df`中的數據會被完整的查出來，但是在`all_emp_df`中又沒有編號為`40` 部門的員工，所以對應的位置都被填入了空值。

```
19	NaN    NaN    NaN    NaN    NaN     NaN    40    運維部    深圳
```

### 數據清洗

通常，我們從 Excel、CSV 或數據庫中獲取到的數據並不是非常完美的，里面可能因為系統或人為的原因混入了重複值或異常值，也可能在某些字段上存在缺失值；再者，`DataFrame`中的數據也可能存在格式不統一、量綱不統一等各種問題。因此，在開始數據分析之前，對數據進行清洗就顯得特別重要。

#### 缺失值

可以使用`DataFrame`對象的`isnull`或`isna`方法來找出數據表中的缺失值，如下所示。

```Python
emp_df.isnull()
```

或者

```Python
emp_df.isna()
```

輸出：

```
        ename   job	    mgr     sal     comm    dno
eno						
1359	False	False	False	False	False	False
2056	False	False	False	False	False	False
3088	False	False	False	False	False	False
3211	False	False	False	False	True	False
3233	False	False	False	False	True	False
3244	False	False	False	False	True	False
3251	False	False	False	False	True	False
3344	False	False	False	False	False	False
3577	False	False	False	False	True	False
3588	False	False	False	False	True	False
4466	False	False	False	False	True	False
5234	False	False	False	False	True	False
5566	False	False	False	False	False	False
7800	False	False	True	False	False	False
```

相對應的，`notnull`和`notna`方法可以將非空的值標記為`True`。如果想删除這些缺失值，可以使用`DataFrame`對象的`dropna`方法，該方法的`axis`參數可以指定沿着0軸還是1軸删除，也就是說當遇到空值時，是删除整行還是删除整列，默認是沿0軸進行删除的，代碼如下所示。

```Python
emp_df.dropna()
```

輸出：

```
        ename   job      mgr	 sal    comm     dno
eno						
1359	胡一刀  銷售員	3344.0	1800   200.0	30
2056	喬峰    架構師	 7800.0	 5000	1500.0	 20
3088	李莫愁  設計師	2056.0	3500   800.0	20
3344	黃蓉    銷售主管	7800.0	3000   800.0	30
5566	宋遠橋  會計師	7800.0	4000   1000.0	10
```

如果要沿着1軸進行删除，可以使用下面的代碼。

```Python
emp_df.dropna(axis=1)
```

輸出：

```
        ename    job      sal    dno
eno				
1359	胡一刀   銷售員    1800	30
2056	喬峰     架構師	  5000	 20
3088	李莫愁   設計師    3500	20
3211	張無忌   程序員    3200	20
3233	丘處機   程序員    3400	20
3244	歐陽鋒   程序員    3200	20
3251	張翠山   程序員    4000	20
3344	黃蓉     銷售主管  3000	30
3577	楊過     會計	   2200	  10
3588	朱九真   會計	  2500	 10
4466	苗人鳳   銷售員	 2500   30
5234	郭靖     出納      2000   10
5566	宋遠橋   會計師    4000   10
7800	張三豐   總裁      9000   20
```

> **注意**：`DataFrame`對象的很多方法都有一個名為`inplace`的參數，該參數的默認值為`False`，表示我們的操作不會修改原來的`DataFrame`對象，而是將處理後的結果通過一個新的`DataFrame`對象返回。如果將該參數的值設置為`True`，那麼我們的操作就會在原來的`DataFrame`上面直接修改，方法的返回值為`None`。簡單的說，上面的操作並沒有修改`emp_df`，而是返回了一個新的`DataFrame`對象。

在某些特定的場景下，我們可以對空值進行填充，對應的方法是`fillna`，填充空值時可以使用指定的值（通過`value`參數進行指定），也可以用表格中前一個單元格（通過設置參數`method=ffill`）或後一個單元格（通過設置參數`method=bfill`）的值進行填充，當代碼如下所示。

```Python
emp_df.fillna(value=0)
```

> **注意**：填充的值如何選擇也是一個值得探討的話題，實際工作中，可能會使用某種統計量（如：均值、眾數等）進行填充，或者使用某種插值法（如：隨機插值法、拉格朗日插值法等）進行填充，甚至有可能通過回歸模型、貝葉斯模型等對缺失數據進行填充。

輸出：

```
        ename    job        mgr      sal     comm    dno
eno
1359	胡一刀    銷售員	   3344.0	1800	200.0	30
2056	喬峰	    分析師	    7800.0	 5000	 1500.0	 20
3088	李莫愁	   設計師	   2056.0	3500	800.0	20
3211	張無忌	   程序員	   2056.0	3200	0.0     20
3233	丘處機	   程序員	   2056.0	3400	0.0	    20
3244	歐陽鋒	   程序員	   3088.0	3200	0.0     20
3251	張翠山	   程序員	   2056.0	4000	0.0	    20
3344	黃蓉	    銷售主管   7800.0	3000	800.0	30
3577	楊過	    會計	     5566.0	  2200	  0.0	  10
3588	朱九真	   會計	    5566.0	 2500	 0.0	 10
4466	苗人鳳	   銷售員	   3344.0	2500	0.0	    30
5234	郭靖	    出納	     5566.0	  2000	  0.0	  10
5566	宋遠橋	   會計師	   7800.0	4000	1000.0	10
7800	張三豐	   總裁	    0.0      9000	 1200.0	 20
```

#### 重複值

接下來，我們先給之前的部門表添加兩行數據，讓部門表中名為“研發部”和“銷售部”的部門各有兩個。

```Python
dept_df.loc[50] = {'dname': '研發部', 'dloc': '上海'}
dept_df.loc[60] = {'dname': '銷售部', 'dloc': '長沙'}
dept_df
```

輸出:

```
    dname  dloc
dno		
10	會計部	北京
20	研發部	成都
30	銷售部	重慶
40	運維部	天津
50	研發部	上海
60	銷售部	長沙
```

現在，我們的數據表中有重複數據了，我們可以通過`DataFrame`對象的`duplicated`方法判斷是否存在重複值，該方法在不指定參數時默認判斷行索引是否重複，我們也可以指定根據部門名稱`dname`判斷部門是否重複，代碼如下所示。

```Python
dept_df.duplicated('dname')
```

輸出：

```
dno
10    False
20    False
30    False
40    False
50     True
60     True
dtype: bool
```

從上面的輸出可以看到，`50`和`60`兩個部門從部門名稱上來看是重複的，如果要删除重複值，可以使用`drop_duplicates`方法，該方法的`keep`參數可以控制在遇到重複值時，保留第一項還是保留最後一項，或者多個重複項一個都不用保留，全部删除掉。

```Python
dept_df.drop_duplicates('dname')
```

輸出：

```
	dname	dloc
dno		
10	會計部	北京
20	研發部	成都
30	銷售部	重慶
40	運維部	天津
```

將`keep`參數的值修改為`last`。

```Python
dept_df.drop_duplicates('dname', keep='last')
```

輸出：

```
	dname	dloc
dno		
10	會計部	北京
40	運維部	天津
50	研發部	上海
60	銷售部	長沙
```

使用同樣的方式，我們也可以清除`all_emp_df`中的重複數據，例如我們認定“ename”和“job”兩個字段完全相同的就是重複數據，我們可以用下面的代碼去除重複數據。

```python
all_emp_df.drop_duplicates(['ename', 'job'], inplace=True)
```

> **說明**：上面的`drop_duplicates`方法添加了參數`inplace=True`，該方法不會返回新的`DataFrame`對象，而是在原來的`DataFrame`對象上直接删除，大家可以查看`all_emp_df`看看是不是已經移除了重複的員工數據。

#### 異常值

異常值在統計學上的全稱是疑似異常值，也稱作離群點（outlier），異常值的分析也稱作離群點分析。異常值是指樣本中出現的“極端值”，數據值看起來異常大或異常小，其分佈明顯偏離其餘的觀測值。實際工作中，有些異常值可能是由系統或人為原因造成的，但有些異常值卻不是，它們能夠重複且穩定的出現，屬於正常的極端值，例如很多遊戲產品中頭部玩家的數據往往都是離群的極端值。所以，我們既不能忽視異常值的存在，也不能簡單地把異常值從數據分析中剔除。重視異常值的出現，分析其產生的原因，常常成為發現問題進而改進決策的契機。

異常值的檢測有Z-score 方法、IQR 方法、DBScan 聚類、孤立森林等，這里我們對前兩種方法做一個簡單的介紹。

<img src="http://localhost/mypic/20211004192858.png" style="zoom:50%;">

如果數據服從正態分佈，依據3σ法則，異常值被定義與平均值的偏差超過三倍標準差的值。在正態分佈下，距離平均值3σ之外的值出現的概率為$ P(|x-\mu|>3\sigma)<0.003 $，屬於小概率事件。如果數據不服從正態分佈，那麼可以用遠離均值的多少倍的標準差來描述，這里的倍數就是Z-score。Z-score以標準差為單位去度量某一原始分數偏離平均值的距離，公式如下所示。
$$
z = \frac {X - \mu} {\sigma} \\
|z| > 3
$$
Z-score需要根據經驗和實際情況來決定，通常把遠離標準差`3`倍距離以上的數據點視為離群點，下面的代給出了如何通過Z-score方法檢測異常值。

```Python
def detect_outliers_zscore(data, threshold=3):
    avg_value = np.mean(data)
    std_value = np.std(data)
    z_score = np.abs((data - avg_value) / std_value)
    return data[z_score > threshold]
```

IQR 方法中的IQR（Inter-Quartile Range）代表四分位距離，即上四分位數（Q3）和下四分位數（Q1）的差值。通常情況下，可以認為小於 $ Q1 - 1.5 \times IQR $ 或大於 $ Q3 + 1.5 \times IQR $ 的就是異常值，而這種檢測異常值的方法也是箱線圖（後面會講到）默認使用的方法。下面的代碼給出了如何通過 IQR 方法檢測異常值。

```Python
def detect_outliers_iqr(data, whis=1.5):
    q1, q3 = np.quantile(data, [0.25, 0.75])
    iqr = q3 - q1
    lower, upper = q1 - whis * iqr, q3 + whis * iqr
    return data[(data < lower) | (data > upper)]
```

如果要删除異常值，可以使用`DataFrame`對象的`drop`方法，該方法可以根據行索引或列索引删除指定的行或列。例如我們認為月薪低於`2000`或高於`8000`的是員工表中的異常值，可以用下面的代碼删除對應的記錄。

```Python
emp_df.drop(emp_df[(emp_df.sal > 8000) | (emp_df.sal < 2000)].index)
```

如果要替換掉異常值，可以通過給單元格賦值的方式來實現，也可以使用`replace`方法將指定的值替換掉。例如我們要將月薪為`1800`和`9000`的替換為月薪的平均值，補貼為`800`的替換為`1000`，代碼如下所示。

```Python
avg_sal = np.mean(emp_df.sal).astype(int)
emp_df.replace({'sal': [1800, 9000], 'comm': 800}, {'sal': avg_sal, 'comm': 1000})
```

#### 預處理

對數據進行預處理也是一個很大的話題，它包含了對數據的拆解、變換、歸約、離散化等操作。我們先來看看數據的拆解。如果數據表中的數據是一個時間日期，我們通常都需要從年、季度、月、日、星期、小時、分鐘等維度對其進行拆解，如果時間日期是用字符串表示的，可以先通過`pandas`的`to_datetime`函數將其處理成時間日期。

在下面的例子中，我們先讀取 Excel 文件，獲取到一組銷售數據，其中第一列就是銷售日期，我們將其拆解為“月份”、“季度”和“星期”，代碼如下所示。

```Python
sales_df = pd.read_excel(
    'data/2020年銷售數據.xlsx',
    usecols=['銷售日期', '銷售區域', '銷售渠道', '品牌', '銷售額']
)
sales_df.info()
```

> **說明**：上面代碼中使用了相對路徑來獲取 Excel 文件，也就是說 Excel 文件在當前工作路徑下名為`data`的文件夾中。如果需要上面例子中的 Excel 文件，可以通過下面的百度雲盤地址進行獲取。鏈接：<https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g>，提取碼：e7b4。

輸出：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1945 entries, 0 to 1944
Data columns (total 5 columns):
 #   Column  Non-Null Count  Dtype         
---  ------  --------------  -----         
 0   銷售日期    1945 non-null   datetime64[ns]
 1   銷售區域    1945 non-null   object        
 2   銷售渠道    1945 non-null   object        
 3   品牌        1945 non-null   object        
 4   銷售額      1945 non-null   int64         
dtypes: datetime64[ns](1), int64(1), object(3)
memory usage: 76.1+ KB
```

```Python
sales_df['月份'] = sales_df['銷售日期'].dt.month
sales_df['季度'] = sales_df['銷售日期'].dt.quarter
sales_df['星期'] = sales_df['銷售日期'].dt.weekday
sales_df
```

輸出：

```
	    銷售日期	 銷售區域	銷售渠道	品牌	  銷售額	月份	季度	星期
0	    2020-01-01	上海	     拼多多	 八匹馬   8217	    1	 1	   2
1	    2020-01-01	上海	     抖音	      八匹馬	6351	 1	  1	    2
2	    2020-01-01	上海	     天貓	      八匹馬	14365	 1	  1	    2
3	    2020-01-01	上海	     天貓       八匹馬	2366	 1	  1     2
4	    2020-01-01	上海	     天貓 	  皮皮蝦	15189	 1	  1     2
...     ...         ...        ...       ...      ...     ...  ...   ...
1940    2020-12-30	北京	     京東	      花花姑娘 6994     12	 4	   2
1941    2020-12-30	福建	     實體	      八匹馬	7663	 12	  4	    2
1942    2020-12-31	福建	     實體	      花花姑娘 14795    12	 4	   3
1943    2020-12-31	福建	     抖音	      八匹馬	3481	 12	  4	    3
1944    2020-12-31	福建	     天貓	      八匹馬	2673	 12	  4	    3
```

在上面的代碼中，通過日期時間類型的`Series`對象的`dt` 屬性，獲得一個訪問日期時間的對象，通過該對象的`year`、`month`、`quarter`、`hour`等屬性，就可以獲取到年、月、季度、小時等時間信息，獲取到的仍然是一個`Series`對象，它包含了一組時間信息，所以我們通常也將這個`dt`屬性稱為“日期時間向量”。

我們再來說一說字符串類型的數據的處理，我們先從指定的 Excel 文件中讀取某招聘網站的招聘數據。

```Python
jobs_df = pd.read_csv(
    'data/某招聘網站招聘數據.csv',
    usecols=['city', 'companyFullName', 'positionName', 'salary']
)
jobs_df.info()
```

> **說明**：上面代碼中使用了相對路徑來獲取 CSV 文件，也就是說 CSV 文件在當前工作路徑下名為`data`的文件夾中。如果需要上面例子中的 CSV 文件，可以通過下面的百度雲盤地址進行獲取。鏈接：<https://pan.baidu.com/s/1rQujl5RQn9R7PadB2Z5g_g>，提取碼：e7b4。

輸出：

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 3140 entries, 0 to 3139
Data columns (total 4 columns):
 #   Column           Non-Null Count  Dtype 
---  ------           --------------  ----- 
 0   city             3140 non-null   object
 1   companyFullName  3140 non-null   object
 2   positionName     3140 non-null   object
 3   salary           3140 non-null   object
dtypes: object(4)
memory usage: 98.2+ KB
```

查看前`5`條數據。

```Python
jobs_df.head()
```

輸出：

```
    city    companyFullName              positionName    salary
0   北京	  達疆網絡科技（上海）有限公司    數據分析崗       15k-30k
1   北京	  北京音娛時光科技有限公司        數據分析        10k-18k
2   北京	  北京千喜鶴餐飲管理有限公司	     數據分析        20k-30k
3   北京	  吉林省海生電子商務有限公司	     數據分析        33k-50k
4   北京	  韋博網訊科技（北京）有限公司	數據分析        10k-15k
```

上面的數據表一共有`3140`條數據，但並非所有的職位都是“數據分析”的崗位，如果要篩選出數據分析的崗位，可以通過檢查`positionName`字段是否包含“數據分析”這個關鍵詞，這里需要模糊匹配，應該如何實現呢？我們可以先獲取`positionName`列，因為這個`Series`對象的`dtype`是字符串，所以可以通過`str`屬性獲取對應的字符串向量，然後就可以利用我們熟悉的字符串的方法來對其進行操作，代碼如下所示。

```Python
jobs_df = jobs_df[jobs_df.positionName.str.contains('數據分析')]
jobs_df.shape
```

輸出：

```
(1515, 4)
```

可以看出，篩選後的數據還有`1515`條。接下來，我們還需要對`salary`字段進行處理，如果我們希望統計所有崗位的平均工資或每個城市的平均工資，首先需要將用範圍表示的工資處理成其中間值，代碼如下所示。

```Python
jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?')
```

> **說明**：上面的代碼通過正則表達式捕獲組從字符串中抽取出兩組數字，分別對應工資的下限和上限，對正則表達式不熟悉的讀者，可以閱讀我的知乎專欄“從零開始學Python”中的[《正則表達式的應用》](https://zhuanlan.zhihu.com/p/158929767)一文。

輸出：

```
        0     1
0	    15    30
1	    10	  18
2       20    30
3       33    50
4       10    15
...     ...   ...
3065    8     10
3069    6     10
3070    2     4
3071    6     12
3088    8     12
```

需要提醒大家的是，抽取出來的兩列數據都是字符串類型的值，我們需要將其轉換成`int`類型，才能計算平均值，對應的方法是`DataFrame`對象的`applymap`方法，該方法的參數是一個函數，而該函數會作用於`DataFrame`中的每個元素。完成這一步之後，我們就可以使用`apply`方法將上面的`DataFrame`處理成中間值，`apply`方法的參數也是一個函數，可以通過指定`axis`參數使其作用於`DataFrame` 對象的行或列，代碼如下所示。

```Python
temp_df = jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?').applymap(int)
temp_df.apply(np.mean, axis=1)
```

 輸出：

```
0       22.5
1       14.0
2       25.0
3       41.5
4       12.5
        ... 
3065    9.0
3069    8.0
3070    3.0
3071    9.0
3088    10.0
Length: 1515, dtype: float64
```

接下來，我們可以用上面的結果替換掉原來的`salary`列或者增加一個新的列來表示職位對應的工資，完整的代碼如下所示。

```Python
temp_df = jobs_df.salary.str.extract(r'(\d+)[kK]?-(\d+)[kK]?').applymap(int)
jobs_df['salary'] = temp_df.apply(np.mean, axis=1)
jobs_df.head()
```

輸出：

```
    city    companyFullName              positionName    salary
0   北京	  達疆網絡科技（上海）有限公司    數據分析崗       22.5
1   北京	  北京音娛時光科技有限公司        數據分析        14.0
2   北京	  北京千喜鶴餐飲管理有限公司	     數據分析        25.0
3   北京	  吉林省海生電子商務有限公司	     數據分析        41.5
4   北京	  韋博網訊科技（北京）有限公司	數據分析        12.5
```

`applymap`和`apply`兩個方法在數據預處理的時候經常用到，`Series`對象也有`apply`方法，也是用於數據的預處理，但是`DataFrame`對象還有一個名為`transform` 的方法，也是通過傳入的函數對數據進行變換，類似`Series`對象的`map`方法。需要強調的是，`apply`方法具有歸約效果的，簡單的說就是能將較多的數據處理成較少的數據或一條數據；而`transform`方法沒有歸約效果，只能對數據進行變換，原來有多少條數據，處理後還是有多少條數據。

如果要對數據進行深度的分析和挖掘，字符串、日期時間這樣的非數值類型都需要處理成數值，因為非數值類型沒有辦法計算相關性，也沒有辦法進行$\chi^2$檢驗等操作。對於字符串類型，通常可以其分為以下三類，再進行對應的處理。

1. 有序變量（Ordinal Variable）：字符串表示的數據有順序關系，那麼可以對字符串進行序號化處理。
2. 分類變量（Categorical Variable）/ 名義變量（Nominal Variable）：字符串表示的數據沒有大小關系和等級之分，那麼就可以使用獨熱編碼的方式處理成啞變量（虛擬變量）矩陣。
3. 定距變量（Scale Variable）：字符串本質上對應到一個有大小高低之分的數據，而且可以進行加減運算，那麼只需要將字符串處理成對應的數值即可。

對於第1類和第3類，我們可以用上面提到的`apply`或`transform`方法來處理，也可以利用`scikit-learn`中的`OrdinalEncoder`處理第1類字符串，這個我們在後續的課程中會講到。對於第2類字符串，可以使用`pandas`的`get_dummies()`函數來生成啞變量（虛擬變量）矩陣，代碼如下所示。

```Python
persons_df = pd.DataFrame(
    data={
        '姓名': ['關羽', '張飛', '趙雲', '馬超', '黃忠'],
        '職業': ['醫生', '醫生', '程序員', '畫家', '教師'],
        '學歷': ['研究生', '大專', '研究生', '高中', '本科']
    }
)
persons_df
```

輸出：

```
	姓名	職業	學歷
0	關羽	醫生	研究生
1	張飛	醫生	大專
2	趙雲	程序員	研究生
3	馬超	畫家	高中
4	黃忠	教師	本科
```

將職業處理成啞變量矩陣。

```Python
pd.get_dummies(persons_df['職業'])
```

輸出：

```
    醫生 教師  畫家  程序員
0	1    0    0    0
1	1    0    0    0
2	0    0    0    1
3	0    0    1    0
4	0    1    0    0
```

將學歷處理成大小不同的值。

```Python
def handle_education(x):
    edu_dict = {'高中': 1, '大專': 3, '本科': 5, '研究生': 10}
    return edu_dict.get(x, 0)


persons_df['學歷'].apply(handle_education)
```

輸出：

```
0    10
1     3
2    10
3     1
4     5
Name: 學歷, dtype: int64
```

我們再來說說數據離散化。離散化也叫分箱，如果變量的取值是連續值，那麼它的取值有無數種可能，在進行數據分組的時候就會非常的不方便，這個時候將連續變量離散化就顯得非常重要。之所以把離散化叫做分箱，是因為我們可以預先設置一些箱子，每個箱子代表了數據取值的範圍，這樣就可以將連續的值分配到不同的箱子中，從而實現離散化。下面的例子讀取了2018年北京積分落戶數據，我們可以根據落戶積分對數據進行分組，具體的做法如下所示。

```Python
luohu_df = pd.read_csv('data/2018年北京積分落戶數據.csv', index_col='id')
luohu_df.score.describe()
```

輸出：

```
count    6019.000000
mean       95.654552
std         4.354445
min        90.750000
25%        92.330000
50%        94.460000
75%        97.750000
max       122.590000
Name: score, dtype: float64
```

可以看出，落戶積分的最大值是`122.59`，最小值是`90.75`，那麼我們可以構造一個從`90`分到`125`分，每`5`分一組的`7`個箱子，`pandas`的`cut`函數可以幫助我們首先數據分箱，代碼如下所示。

```Python
bins = np.arange(90, 126, 5)
pd.cut(luohu_df.score, bins, right=False)
```

> **說明**：`cut`函數的`right`參數默認值為`True`，表示箱子左開右閉；修改為`False`可以讓箱子的右邊界為開區間，左邊界為閉區間，大家看看下面的輸出就明白了。

輸出：

```
id
1       [120, 125)
2       [120, 125)
3       [115, 120)
4       [115, 120)
5       [115, 120)
           ...    
6015      [90, 95)
6016      [90, 95)
6017      [90, 95)
6018      [90, 95)
6019      [90, 95)
Name: score, Length: 6019, dtype: category
Categories (7, interval[int64, left]): [[90, 95) < [95, 100) < [100, 105) < [105, 110) < [110, 115) < [115, 120) < [120, 125)]
```

我們可以根據分箱的結果對數據進行分組，然後使用聚合函數對每個組進行統計，這是數據分析中經常用到的操作，下一個章節會為大家介紹。除此之外，`pandas`還提供了一個名為`qcut`的函數，可以指定分位數對數據進行分箱，有興趣的讀者可以自行研究。

