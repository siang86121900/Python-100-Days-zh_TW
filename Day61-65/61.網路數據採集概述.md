## 網路數據採集概述

爬蟲（crawler）也經常被稱為網路蜘蛛（spider），是按照一定的規則自動瀏覽網站並獲取所需信息的機器人程式（自動化腳本代碼），被廣泛的應用於網際網路搜索引擎和數據採集。使用過網際網路和瀏覽器的人都知道，網頁中除了供用戶閱讀的文字信息之外，還包含一些超鏈接，網路爬蟲正是通過網頁中的超鏈接信息，不斷獲得網路上其它頁麵的地址，然後持續的進行數據採集。正因如此，網路數據採集的過程就像一個爬蟲或者蜘蛛在網路上漫遊，所以才被形象的稱為爬蟲或者網路蜘蛛。

### 爬蟲的應用領域

在理想的狀態下，所有 ICP（Internet Content Provider）都應該為自己的網站提供 API 接口來共享它們允許其他程式獲取的數據，在這種情況下就根本不需要爬蟲程式。國內比較有名的電商平臺（如淘寶、京東等）、社交平臺（如微博、微信等）等都提供了自己的 API 接口，但是這類 API 接口通常會對可以抓取的數據以及抓取數據的頻率進行限製。對於大多數的公司而言，及時的獲取行業數據和競對數據是企業生存的重要環節之一，然而對大部分企業來說，數據都是其與生俱來的短闆。在這種情況下，合理的利用爬蟲來獲取數據並從中提取出有商業價值的信息對這些企業來說就顯得至關重要的。

爬蟲的應用領域其實非常廣泛，下麵我們列舉了其中的一部分，有興趣的讀者可以自行探索相關內容。

1. 搜索引擎
2. 新聞聚合
3. 社交應用
4. 輿情監控
5. 行業數據

### 爬蟲合法性探討

經常聽人說起“爬蟲寫得好，牢飯吃到飽”，那麼編程爬蟲程式是否違法呢？關於這個問題，我們可以從以下幾個角度進行解讀。

1. 網路爬蟲這個領域目前還屬於拓荒階段，雖然網際網路世界已經通過自己的遊戲規則建立起了一定的道德規範，即 Robots 協議（全稱是“網路爬蟲排除標準”），但法律部分還在建立和完善中，也就是說，現在這個領域暫時還是灰色地帶。
2. “法不禁止即為許可”，如果爬蟲就像瀏覽器一樣獲取的是前端顯示的數據（網頁上的公開信息）而不是網站後臺的私密敏感信息，就不太擔心法律法規的約束，因為目前大數據産業鏈的發展速度遠遠超過了法律的完善程度。
3. 在爬取網站的時候，需要限製自己的爬蟲遵守 Robots 協議，同時控製網路爬蟲程式的抓取數據的速度；在使用數據的時候，必須要尊重網站的智慧産權（從Web 2.0時代開始，雖然Web上的數據很多都是由用戶提供的，但是網站平臺是投入了營運成本的，當用戶在註冊和PO內容時，平臺通常就已經獲得了對數據的所有權、使用權和分發權）。如果違反了這些規定，在打官司的時候敗訴幾率相當高。
4. 適當的隱匿自己的身份在編寫爬蟲程式時必要的，而且最好不要被對方舉證你的爬蟲有破壞別人動産（例如服務器）的行為。
5. 不要在公網（如代碼托管平臺）上去開源或者展示你的爬蟲代碼，這些行為通常會給自己帶來不必要的麻煩。

#### Robots協議

大多數網站都會定義`robots.txt`文件，這是一個君子協議，並不是所有爬蟲都必須遵守的遊戲規則。下麵以淘寶的[`robots.txt`](http://www.taobao.com/robots.txt)文件為例，看看淘寶網對爬蟲有哪些限製。

```
User-agent: Baiduspider
Disallow: /

User-agent: baiduspider
Disallow: /
```

通過上麵的文件可以看出，淘寶禁止百度爬蟲爬取它任何資源，因此當你在百度搜索“淘寶”的時候，搜索結果下方會出現：“由於該網站的`robots.txt`文件存在限製指令（限製搜索引擎抓取），係統無法提供該頁麵的內容描述”。百度作為一個搜索引擎，至少在錶麵上遵守了淘寶網的`robots.txt`協議，所以用戶不能從百度上搜索到淘寶內部的産品信息。

圖1. 百度搜索淘寶的結果

![](https://gitee.com/jackfrued/mypic/raw/master/20210824004320.png)

下麵是豆瓣網的[`robots.txt`](https://www.douban.com/robots.txt)文件，大家可以自行解讀，看看它做出了什麼樣的限製。

```
User-agent: *
Disallow: /subject_search
Disallow: /amazon_search
Disallow: /search
Disallow: /group/search
Disallow: /event/search
Disallow: /celebrities/search
Disallow: /location/drama/search
Disallow: /forum/
Disallow: /new_subject
Disallow: /service/iframe
Disallow: /j/
Disallow: /link2/
Disallow: /recommend/
Disallow: /doubanapp/card
Disallow: /update/topic/
Disallow: /share/
Allow: /ads.txt
Sitemap: https://www.douban.com/sitemap_index.xml
Sitemap: https://www.douban.com/sitemap_updated_index.xml
# Crawl-delay: 5

User-agent: Wandoujia Spider
Disallow: /

User-agent: Mediapartners-Google
Disallow: /subject_search
Disallow: /amazon_search
Disallow: /search
Disallow: /group/search
Disallow: /event/search
Disallow: /celebrities/search
Disallow: /location/drama/search
Disallow: /j/
```

### 超文本傳輸協議（HTTP）

在開始講解爬蟲之前，我們稍微對超文本傳輸協議（HTTP）做一些回顧，因為我們在網頁上看到的內容通常是瀏覽器執行 HTML （超文本標記語言）得到的結果，而 HTTP 就是傳輸 HTML 數據的協議。HTTP 和其他很多應用級協議一樣是構建在 TCP（傳輸控製協議）之上的，它利用了 TCP 提供的可靠的傳輸服務實現了 Web 應用中的數據交換。按照維基百科上的介紹，設計 HTTP 最初的目的是為了提供一種PO和接收 [HTML](https://zh.wikipedia.org/wiki/HTML) 頁麵的方法，也就是說，這個協議是瀏覽器和 Web 服務器之間傳輸的數據的載體。關於 HTTP 的詳細信息以及目前的發展狀況，大家可以閱讀[《HTTP 協議入門》](http://www.ruanyifeng.com/blog/2016/08/http.html)、[《網際網路協議入門》](http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html)、[《圖解 HTTPS 協議》](http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html)等文章進行了解。

下圖是我在四川省網路通信技術重點實驗室工作期間用開源協議分析工具 Ethereal（WireShark 的前身）截取的訪問百度首頁時的 HTTP 請求和回響的報文（協議數據），由於 Ethereal 截取的是經過網路適配器的數據，因此可以清晰的看到從物理鏈路層到應用層的協議數據。

圖2. HTTP請求

![http-request](https://gitee.com/jackfrued/mypic/raw/master/20210824003915.png)

HTTP 請求通常是由請求行、請求頭、空行、消息體四個部分構成，如果冇有數據發給服務器，消息體就不是必須的部分。請求行中包含了請求方法（GET、POST 等，如下錶所示）、資源路徑和協議版本；請求頭由若幹鍵值對構成，包含了瀏覽器、編碼方式、首選語言、緩存策略等信息；請求頭的後麵是空行和消息體。

<img src="https://gitee.com/jackfrued/mypic/raw/master/20210825002720.PNG" width="65%">

圖3. HTTP回響

![http-response](https://gitee.com/jackfrued/mypic/raw/master/20210824234158.png)

HTTP 回響通常是由回響行、回響頭、空行、消息體四個部分構成，其中消息體是服務回響的數據，可能是 HTML 頁麵，也有可能是JSON或二進製數據等。回響行中包含了協議版本和回響狀態碼，回響狀態碼有很多種，常見的如下錶所示。

<img src="https://gitee.com/jackfrued/mypic/raw/master/20210825002802.PNG" width="65%">

#### 相關工具

下麵我們先介紹一些開發爬蟲程式的輔助工具，這些工具相信能幫助你事半功倍。

1. Chrome Developer Tools：穀歌瀏覽器內置的開發者工具。該工具最常用的幾個功能模塊是：

   - 元素（ELements）：用於檢視或修改 HTML 元素的屬性、CSS 屬性、監聽事件等。CSS 可以即時修改，即時顯示，大大方便了開發者調試頁麵。
   - 控製臺（Console）：用於執行免洗代碼，檢視 JavaScript 對象，檢視調試日誌信息或異常信息。控製臺其實就是一個執行 JavaScript 代碼的交互式環境。
   - 源代碼（Sources）：用於檢視頁麵的 HTML 文件源代碼、JavaScript 源代碼、CSS 源代碼，此外最重要的是可以調試 JavaScript 源代碼，可以給代碼添加斷點和單步執行。
   - 網路（Network）：用於 HTTP 請求、HTTP 回響以及與網路連接相關的信息。
   - 應用（Application）：用於檢視瀏覽器在地存儲、後臺任務等內容，在地存儲主要包括Cookie、Local Storage、Session Storage等。

   ![chrome-developer-tools](https://gitee.com/jackfrued/mypic/raw/master/20210824004034.png)

2. Postman：功能強大的網頁調試與 RESTful 請求工具。Postman可以幫助我們類比請求，非常方便的定製我們的請求以及檢視服務器的回響。

   ![postman](https://gitee.com/jackfrued/mypic/raw/master/20210824004048.png)

3. HTTPie：命令行HTTP客戶端。

   安裝。

   ```Bash
   pip install httpie
   ```

   使用。

   ```Bash
   http --header http --header https://movie.douban.com/
   
   HTTP/1.1 200 OK
   Connection: keep-alive
   Content-Encoding: gzip
   Content-Type: text/html; charset=utf-8
   Date: Tue, 24 Aug 2021 16:48:00 GMT
   Keep-Alive: timeout=30
   Server: dae
   Set-Cookie: bid=58h4BdKC9lM; Expires=Wed, 24-Aug-22 16:48:00 GMT; Domain=.douban.com; Path=/
   Strict-Transport-Security: max-age=15552000
   Transfer-Encoding: chunked
   X-Content-Type-Options: nosniff
   X-DOUBAN-NEWBID: 58h4BdKC9lM
   ```

4. `builtwith`庫：識別網站所用技術的工具。

   安裝。

   ```Bash
   pip install builtwith
   ```

   使用。

   ```Python
   import ssl
   
   import builtwith
   
   ssl._create_default_https_context = ssl._create_unverified_context
   print(builtwith.parse('http://www.bootcss.com/'))
   ```

5. `python-whois`庫：查詢網站所有者的工具。

   安裝。

   ```Bash
   pip3 install python-whois
   ```

   使用。

   ```Python
   import whois
   
   print(whois.whois('https://www.bootcss.com'))
   ```

### 爬蟲的基本工作流程

一個基本的爬蟲通常分為數據採集（網頁下載）、數據處理（網頁解析）和數據存儲（將有用的信息持久化）三個部分的內容，當然更為高級的爬蟲在數據採集和處理時會使用並發編程或分佈式技術，這就需要有調度器（安排線程或進程執行對應的任務）、後臺管理程式（監控爬蟲的工作狀態以及檢查數據抓取的結果）等的參與。

![crawler-workflow](https://gitee.com/jackfrued/mypic/raw/master/20210824004107.png)

一般來說，爬蟲的工作流程包括以下幾個步驟：

1. 設定抓取目標（種子頁麵/起始頁麵）並獲取網頁。
2. 當服務器無法訪問時，按照指定的重試次數嘗試重新下載頁麵。
3. 在需要的時候設定用戶代理或隱藏真實IP，否則可能無法訪問頁麵。
4. 對獲取的頁麵進行必要的解碼操作然後抓取出需要的信息。
5. 在獲取的頁麵中通過某種方式（如正則錶達式）抽取出頁麵中的鏈接信息。
6. 對鏈接進行進一步的處理（獲取頁麵並重複上麵的動作）。
7. 將有用的信息進行持久化以備後續的處理。
